{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pickle\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mtx = np.load('data/mtx.npy')\n",
    "dist = np.load('data/dist.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "test_files = glob.glob('test_images/*.jpg')\n",
    "test_imgs = []\n",
    "for file in test_files:\n",
    "    img = cv2.imread(file, cv2.IMREAD_COLOR)\n",
    "    test_imgs.append(np.flip(img, 2)) # opencv uses bgr instead of rgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undistort images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def undistort_img(img):\n",
    "    dst = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "    return dst\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "undistorted_imgs = []\n",
    "for img in test_imgs:\n",
    "    dst = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "    undistorted_imgs.append(dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thresholding (taken from ./Threshold.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def abs_sobel_thresh(img, orient='x', sobel_kernel=3, thresh=(0, 255)):\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    # Apply x or y gradient with the OpenCV Sobel() function\n",
    "    # and take the absolute value\n",
    "    if orient == 'x':\n",
    "        abs_sobel = np.absolute(cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=sobel_kernel))\n",
    "    if orient == 'y':\n",
    "        abs_sobel = np.absolute(cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=sobel_kernel))\n",
    "    # Rescale back to 8 bit integer\n",
    "    scaled_sobel = np.uint8(255*abs_sobel/np.max(abs_sobel))\n",
    "    # Create a copy and apply the threshold\n",
    "    binary_output = np.zeros_like(scaled_sobel)\n",
    "    # Here I'm using inclusive (>=, <=) thresholds, but exclusive is ok too\n",
    "    binary_output[(scaled_sobel >= thresh[0]) & (scaled_sobel <= thresh[1])] = 1\n",
    "\n",
    "    # Return the result\n",
    "    return binary_output\n",
    "\n",
    "def mag_thresh(img, sobel_kernel=3, mag_thresh=(0, 255)):\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    # Take both Sobel x and y gradients\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=sobel_kernel)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=sobel_kernel)\n",
    "    # Calculate the gradient magnitude\n",
    "    gradmag = np.sqrt(sobelx**2 + sobely**2)\n",
    "    # Rescale to 8 bit\n",
    "    scale_factor = np.max(gradmag)/255 \n",
    "    gradmag = (gradmag/scale_factor).astype(np.uint8) \n",
    "    # Create a binary image of ones where threshold is met, zeros otherwise\n",
    "    binary_output = np.zeros_like(gradmag)\n",
    "    binary_output[(gradmag >= mag_thresh[0]) & (gradmag <= mag_thresh[1])] = 1\n",
    "\n",
    "    # Return the binary image\n",
    "    return binary_output\n",
    "\n",
    "def dir_threshold(img, sobel_kernel=3, thresh=(0, np.pi/2)):\n",
    "    # Grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    # Calculate the x and y gradients\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=sobel_kernel)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=sobel_kernel)\n",
    "    # Take the absolute value of the gradient direction, \n",
    "    # apply a threshold, and create a binary image result\n",
    "    absgraddir = np.arctan2(np.absolute(sobely), np.absolute(sobelx))\n",
    "    binary_output =  np.zeros_like(absgraddir)\n",
    "    binary_output[(absgraddir >= thresh[0]) & (absgraddir <= thresh[1])] = 1\n",
    "\n",
    "    # Return the binary image\n",
    "    return binary_output\n",
    "\n",
    "def combined_sobel_thresh(image):\n",
    "    # Choose a Sobel kernel size\n",
    "    ksize = 3 # Choose a larger odd number to smooth gradient measurements\n",
    "\n",
    "    # Apply each of the thresholding functions\n",
    "    gradx = abs_sobel_thresh(image, orient='x', sobel_kernel=ksize, thresh=(20, 100))\n",
    "    grady = abs_sobel_thresh(image, orient='y', sobel_kernel=ksize, thresh=(20, 100))\n",
    "    mag_binary = mag_thresh(image, sobel_kernel=ksize, mag_thresh=(30, 100))\n",
    "    dir_binary = dir_threshold(image, sobel_kernel=ksize, thresh=(.7, 1.3))\n",
    "\n",
    "    combined = np.zeros_like(dir_binary)\n",
    "    combined[((gradx == 1) & (grady == 1)) | ((mag_binary == 1) & (dir_binary == 1))] = 1\n",
    "    return combined.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def img_histogram_eq(img):\n",
    "    img_yuv = cv2.cvtColor(img, cv2.COLOR_BGR2YUV)\n",
    "\n",
    "    # equalize the histogram of the Y channel\n",
    "    img_yuv[:,:,0] = cv2.equalizeHist(img_yuv[:,:,0])\n",
    "\n",
    "    # convert the YUV image back to RGB format\n",
    "    img_output = cv2.cvtColor(img_yuv, cv2.COLOR_YUV2BGR)\n",
    "    \n",
    "    return img_output\n",
    "\n",
    "def color_threshold(img, color_model, low, high):\n",
    "    hls = cv2.cvtColor(img, color_model)\n",
    "    \n",
    "    channel_thresh_img = np.zeros_like(img)\n",
    "    hsv_binary = np.zeros(img.shape[:1])\n",
    "    for i in range(img.shape[-1]):\n",
    "        idx = (hls[...,i] >= low[i]) & (hls[...,i] <= high[i])\n",
    "        channel_thresh_img[idx, i] = 1\n",
    "    hsv_binary = channel_thresh_img[..., 0] & channel_thresh_img[..., 1] & channel_thresh_img[..., 2]\n",
    "    return hsv_binary\n",
    "\n",
    "def combined_color_thresh(img):\n",
    "    hls_img_y = color_threshold(img, cv2.COLOR_RGB2HLS, [0,100,90], [40,255,255])\n",
    "    lab_img_y = color_threshold(img, cv2.COLOR_RGB2LAB, [0,0,155], [255,255,200])\n",
    "    luv_img_w = color_threshold(img, cv2.COLOR_RGB2LUV, [210,0,0], [255,255,255])\n",
    "    \n",
    "    combined = np.zeros_like(hls_img_y)\n",
    "    for img in [hls_img_y, lab_img_y, luv_img_w]:\n",
    "        combined = combined | img\n",
    "    return combined.astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combined_thresh(sobel_binary, color_binary):\n",
    "    # Adding more weight to color binary, because it is better at finding yellow/white lines\n",
    "    combined_binary = sobel_binary + color_binary * 8\n",
    "    return combined_binary.astype(np.uint8)\n",
    "\n",
    "def threshold_image(img):\n",
    "    sobel_binary = combined_sobel_thresh(img)\n",
    "    color_binary = combined_color_thresh(img)\n",
    "    return combined_thresh(sobel_binary, color_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_polygon(img, vertices, color=[255, 0, 0], thickness=2):\n",
    "    img = img.copy()\n",
    "    vertices = np.array(vertices, dtype=np.int32)\n",
    "    for i in range(len(vertices)):\n",
    "        cv2.line(img, tuple(vertices[i-1]), tuple(vertices[i]), color, thickness)\n",
    "    return img\n",
    "    \n",
    "def get_src(img):\n",
    "    h, w = img.shape[:2]\n",
    "    mid1, mid2 = [(w*.408), (h*.7)], [(w*.6), (h*.7)]\n",
    "    bot1, bot2 = [(w*.16), h], [(w*.865), h]\n",
    "    vertices = [mid1, mid2, bot2, bot1]\n",
    "    return vertices\n",
    "\n",
    "# Extended lane lines\n",
    "def get_src(img):\n",
    "    h, w = img.shape[:2]\n",
    "    mid1, mid2 = [(w*.441), (h*.64)], [(w*.565), (h*.642)]\n",
    "    bot1, bot2 = [(w*.08), h], [(w*.95), h]\n",
    "    vertices = [mid1, mid2, bot2, bot1]\n",
    "    return vertices\n",
    "\n",
    "def get_dst(img):\n",
    "    h, w = img.shape[:2]\n",
    "    offsetx=200\n",
    "    offsety=-10\n",
    "    mid1, mid2 = [offsetx, offsety], [w-offsetx, offsety]\n",
    "    bot1, bot2 = [offsetx, h], [w-offsetx, h]\n",
    "    dst = [mid1, mid2, bot2, bot1]\n",
    "    return dst\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perspective transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def warp_image(img):\n",
    "    src = np.float32(get_src(img))\n",
    "    dst = np.float32(get_dst(img))\n",
    "    h, w = img.shape[:2]\n",
    "    M = cv2.getPerspectiveTransform(src, dst)\n",
    "    Minv = cv2.getPerspectiveTransform(dst, src)\n",
    "    # Warp the image using OpenCV warpPerspective()\n",
    "    warped = cv2.warpPerspective(img, M, (w, h))\n",
    "    return warped, M, Minv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sliding window search with convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# window settings\n",
    "window_width = 70\n",
    "window_height = 90 # Break image into 9 vertical layers since image height is 720\n",
    "margin = 70 # How much to slide left and right for searching\n",
    "window_sum_threshold = 2000\n",
    "\n",
    "from scipy.signal import gaussian\n",
    "\n",
    "def window_mask(width, height, img_ref, center,level):\n",
    "    output = np.zeros_like(img_ref)\n",
    "    output[int(img_ref.shape[0]-(level+1)*height):int(img_ref.shape[0]-level*height),max(0,int(center-width/2)):min(int(center+width/2),img_ref.shape[1])] = 1\n",
    "    return output\n",
    "\n",
    "def find_window_centroids(image, window_width, window_height, margin, prev_l_center=None, prev_r_center=None):\n",
    "    global sums\n",
    "    window_centroids = [] # Store the (left,right) window centroid positions per level\n",
    "    window = gaussian(window_width, std=window_width, sym=True)\n",
    "\n",
    "    # First find the two starting positions for the left and right lane by using np.sum to get the vertical image slice\n",
    "    # and then np.convolve the vertical image slice with the window template \n",
    "    \n",
    "    # Sum quarter bottom of image to get slice, could use a different ratio\n",
    "    def get_centroid_start(y_ratio=1/3, lx_ratio=1/3, rx_ratio=2/3):\n",
    "        l_sum = np.sum(image[int(image.shape[0]*y_ratio):,:int(image.shape[1]*lx_ratio)], axis=0)\n",
    "        l_center = np.argmax(np.convolve(window,l_sum))-window_width/2\n",
    "        \n",
    "        l_conv = np.convolve(window,l_sum)\n",
    "        r_sum = np.sum(image[int(image.shape[0]*y_ratio):,int(image.shape[1]*rx_ratio):], axis=0)\n",
    "        r_conv = np.convolve(window,r_sum)\n",
    "        r_center = np.argmax(np.convolve(window,r_sum))-window_width/2+int(image.shape[1]*rx_ratio)\n",
    "        \n",
    "        \n",
    "        return l_conv, l_center, r_conv, r_center\n",
    "    \n",
    "    l_conv, l_center, r_conv, r_center = get_centroid_start()\n",
    "    \n",
    "    if prev_l_center is not None and prev_r_center is not None:\n",
    "        l_center = prev_l_center\n",
    "        r_center = prev_r_center\n",
    "    \n",
    "    # Go through each layer looking for max pixel locations\n",
    "    for level in range(0,(int)(image.shape[0]/window_height)):\n",
    "        # convolve the window into the vertical slice of the image\n",
    "        image_layer = np.sum(image[int(image.shape[0]-(level+1)*window_height):int(image.shape[0]-level*window_height),:], axis=0)\n",
    "        conv_signal = np.convolve(window, image_layer)\n",
    "        # Find the best left centroid by using past left center as a reference\n",
    "        # Use window_width/2 as offset because convolution signal reference is at right side of window, not center of window\n",
    "        offset = window_width/2\n",
    "        l_min_index = int(max(l_center+offset-margin,0))\n",
    "        l_max_index = int(min(l_center+offset+margin,image.shape[1]))\n",
    "        l_conv = conv_signal[l_min_index:l_max_index]\n",
    "        \n",
    "        if np.max(l_conv) < window_sum_threshold:\n",
    "            pass\n",
    "        else:\n",
    "            l_center = np.argmax(l_conv)+l_min_index-offset\n",
    "        # Find the best right centroid by using past right center as a reference\n",
    "        r_min_index = int(max(r_center+offset-margin,0))\n",
    "        r_max_index = int(min(r_center+offset+margin,image.shape[1]))\n",
    "        r_conv = conv_signal[r_min_index:r_max_index]\n",
    "        if np.max(r_conv) < window_sum_threshold:\n",
    "            pass\n",
    "        else:\n",
    "            r_center = np.argmax(r_conv)+r_min_index-offset\n",
    "        # Add what we found for that layer\n",
    "        window_centroids.append((l_center,r_center))\n",
    "\n",
    "    return window_centroids\n",
    "\n",
    "def draw_centroids(image, window_centroids):\n",
    "    # If we found any window centers\n",
    "    if len(window_centroids) > 0:\n",
    "\n",
    "        # Points used to draw all the left and right windows\n",
    "        l_points = np.zeros_like(image)\n",
    "        r_points = np.zeros_like(image)\n",
    "\n",
    "        # Go through each level and draw the windows \t\n",
    "        for level in range(0,len(window_centroids)):\n",
    "            # Window_mask is a function to draw window areas\n",
    "            l_mask = window_mask(window_width,window_height,image,window_centroids[level][0],level)\n",
    "            r_mask = window_mask(window_width,window_height,image,window_centroids[level][1],level)\n",
    "            # Add graphic points from window mask here to total pixels found \n",
    "            l_points[(l_points == 255) | ((l_mask == 1) ) ] = 255\n",
    "            r_points[(r_points == 255) | ((r_mask == 1) ) ] = 255\n",
    "\n",
    "        # Draw the results\n",
    "        template = np.array(r_points+l_points,np.uint8) # add both left and right window pixels together\n",
    "        zero_channel = np.zeros_like(template) # create a zero color channel\n",
    "        template = np.array(cv2.merge((zero_channel,template,zero_channel)),np.uint8) # make window pixels green\n",
    "        warpage= np.dstack((image, image, image))*255 # making the original road pixels 3 color channels\n",
    "        output = cv2.addWeighted(warpage, 1, template, 0.5, 0.0) # overlay the orignal road image with window results\n",
    "        \n",
    "\n",
    "    # If no window centers found, just display orginal road image\n",
    "    else:\n",
    "        output = np.array(cv2.merge((image,image,image)),np.uint8)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding lane lines - fit quadratic curve to centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_centroid_coordinates(window_centroids):\n",
    "    centy = []\n",
    "    leftx = []\n",
    "    rightx = []\n",
    "    for idx, (l, r) in enumerate(window_centroids):\n",
    "        y = (len(window_centroids) - idx) * window_height - window_height/2\n",
    "        centy.append(y)\n",
    "        leftx.append(l)\n",
    "        rightx.append(r)\n",
    "    return centy, leftx, rightx\n",
    "\n",
    "def fit_centroids(centy, leftx, rightx):\n",
    "    # Fit a second order polynomial to each\n",
    "    left_fit = np.polyfit(centy, leftx, 2)\n",
    "    right_fit = np.polyfit(centy, rightx, 2)\n",
    "    \n",
    "    return left_fit, right_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_coordinates(binary_warped, left_fit, right_fit):\n",
    "    # Generate x and y values for plotting\n",
    "#     ploty = np.linspace(0, binary_warped.shape[0]-1+80, binary_warped.shape[0]+80 )\n",
    "    ploty = np.linspace(-80, binary_warped.shape[0]-1, binary_warped.shape[0]+80 )\n",
    "    left_fitx = left_fit[0]*ploty**2 + left_fit[1]*ploty + left_fit[2]\n",
    "    right_fitx = right_fit[0]*ploty**2 + right_fit[1]*ploty + right_fit[2]\n",
    "    return ploty, left_fitx, right_fitx\n",
    "    \n",
    "def plot_lane_lines(binary_warped, ploty, left_fitx, right_fitx):\n",
    "    plt.figure()\n",
    "    plt.imshow(binary_warped)\n",
    "    plt.plot(left_fitx, ploty, color='yellow')\n",
    "    plt.plot(right_fitx, ploty, color='yellow')\n",
    "    plt.xlim(0, 1280)\n",
    "    plt.ylim(720, 0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring curvature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def curvature_pixels(img, left_fit, right_fit):\n",
    "    # Define y-value where we want radius of curvature\n",
    "    # I'll choose the maximum y-value, corresponding to the bottom of the image\n",
    "    y_eval = img.shape[0]-1\n",
    "    left_curverad = ((1 + (2*left_fit[0]*y_eval + left_fit[1])**2)**1.5) / np.absolute(2*left_fit[0])\n",
    "    right_curverad = ((1 + (2*right_fit[0]*y_eval + right_fit[1])**2)**1.5) / np.absolute(2*right_fit[0])\n",
    "    return left_curverad, right_curverad\n",
    "    # Example values: 1926.74 1908.48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def curvature_meters(img, centy, leftx, rightx):\n",
    "    y_eval = img.shape[0]-1\n",
    "    # Define conversions in x and y from pixels space to meters\n",
    "    ym_per_pix = 30/720 # meters per pixel in y dimension\n",
    "    xm_per_pix = 3.7/700 # meters per pixel in x dimension\n",
    "\n",
    "    left_fit_cr = np.polyfit(np.array(centy)*ym_per_pix, np.array(leftx)*xm_per_pix, 2)\n",
    "    right_fit_cr = np.polyfit(np.array(centy)*ym_per_pix, np.array(rightx)*xm_per_pix, 2)\n",
    "\n",
    "    # Calculate the new radii of curvature\n",
    "    left_curverad = ((1 + (2*left_fit_cr[0]*y_eval*ym_per_pix + left_fit_cr[1])**2)**1.5) / np.absolute(2*left_fit_cr[0])\n",
    "    right_curverad = ((1 + (2*right_fit_cr[0]*y_eval*ym_per_pix + right_fit_cr[1])**2)**1.5) / np.absolute(2*right_fit_cr[0])\n",
    "    \n",
    "    # Now our radius of curvature is in meters\n",
    "    return left_curverad, right_curverad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def meters_off(img, left_fit, right_fit):\n",
    "    xm_per_pix = 3.7/700 # meteres per pixel in x dimension\n",
    "    screen_middel_pixel = img.shape[1]/2\n",
    "    left_lane_pixel = np.poly1d(left_fit)(img.shape[0]-1)\n",
    "    right_lane_pixel = np.poly1d(right_fit)(img.shape[0]-1)\n",
    "    car_middle_pixel = int((right_lane_pixel + left_lane_pixel)/2)\n",
    "    screen_off_center = screen_middel_pixel-car_middle_pixel\n",
    "    meters_off_center = xm_per_pix * screen_off_center\n",
    "    return meters_off_center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overlay onto original image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def overlay_img(warped, ploty, left_fitx, right_fitx, Minv, undist):\n",
    "    # Create an image to draw the lines on\n",
    "    warp_zero = np.zeros_like(warped).astype(np.uint8)\n",
    "    color_warp = np.dstack((warp_zero, warp_zero, warp_zero))\n",
    "\n",
    "    # Recast the x and y points into usable format for cv2.fillPoly()\n",
    "    pts_left = np.array([np.transpose(np.vstack([left_fitx, ploty]))])\n",
    "    pts_right = np.array([np.flipud(np.transpose(np.vstack([right_fitx, ploty])))])\n",
    "    pts = np.hstack((pts_left, pts_right))\n",
    "\n",
    "    # Draw the lane onto the warped blank image\n",
    "    cv2.fillPoly(color_warp, np.int_([pts]), (0,255, 0))\n",
    "\n",
    "    image = warped\n",
    "    # Warp the blank back to original image space using inverse perspective matrix (Minv)\n",
    "    newwarp = cv2.warpPerspective(color_warp, Minv, (image.shape[1], image.shape[0])) \n",
    "    # Combine the result with the original image\n",
    "    result = cv2.addWeighted(undist, 1, newwarp, 0.3, 0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing: Show Process Step By Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_all(img):\n",
    "    # Threshold\n",
    "    combined_binary = threshold_image(img)\n",
    "    plt.figure()\n",
    "    plt.imshow(combined_binary)\n",
    "    \n",
    "    # Guiding lines\n",
    "    src = get_src(img)\n",
    "    poly_img = draw_polygon(img, src)\n",
    "    plt.figure()\n",
    "    plt.imshow(poly_img)\n",
    "\n",
    "    # Perspective warp (original)\n",
    "    poly_warped_img, M, Minv = warp_image(poly_img)\n",
    "    plt.figure()\n",
    "    plt.imshow(poly_warped_img)\n",
    "    \n",
    "    # Perspective warp (threshold)\n",
    "    warped_img, M, Minv = warp_image(combined_binary)\n",
    "    plt.figure()\n",
    "    plt.imshow(warped_img)\n",
    "    \n",
    "    # Sliding window search with centroids\n",
    "    window_centroids = find_window_centroids(warped_img, window_width, window_height, margin)\n",
    "    centroid_img = draw_centroids(warped_img, window_centroids)\n",
    "\n",
    "    # Fit polynomial to find lane lines\n",
    "    centy, leftx, rightx = get_centroid_coordinates(window_centroids)\n",
    "    left_fit, right_fit = fit_centroids(centy, leftx, rightx)\n",
    "    ploty, left_fitx, right_fitx = fit_coordinates(centroid_img, left_fit, right_fit)\n",
    "    plot_lane_lines(centroid_img, ploty, left_fitx, right_fitx)\n",
    "    \n",
    "    linear_leftfit = np.polyfit(centy, leftx, 1)\n",
    "    linear_rightfit = np.polyfit(centy, rightx, 1)\n",
    "    print('Left slope:', linear_leftfit)\n",
    "    print('Right slope:', linear_rightfit)\n",
    "    \n",
    "    left_curverad, right_curverad = curvature_meters(centroid_img, centy, leftx, rightx)\n",
    "    print(left_curverad, 'm', right_curverad, 'm')\n",
    "    \n",
    "    meters_off_center = meters_off(centroid_img, left_fit, right_fit)\n",
    "    print('Meters off center: {}m'.format(meters_off_center))\n",
    "    \n",
    "    overlay = overlay_img(warped_img, ploty, left_fitx, right_fitx, Minv, img)\n",
    "    plt.figure()\n",
    "    plt.imshow(overlay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "broken_images = np.load('broken_images/broken_challenge_images.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " run_all(undistorted_imgs[5])\n",
    "# run_all(broken_images[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Line object to save previous values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Line():\n",
    "    def __init__(self):\n",
    "        # was the line detected in the last iteration?\n",
    "        self.detected = False  \n",
    "        # x values of the last n fits of the line\n",
    "        self.recent_xfitted = [] \n",
    "        #average x values of the fitted line over the last n iterations\n",
    "        self.bestx = None     \n",
    "        #polynomial coefficients averaged over the last n iterations\n",
    "        self.best_fit = None  \n",
    "        #polynomial coefficients for the most recent fit\n",
    "        self.current_fit = [np.array([False])]  \n",
    "        #difference in fit coefficients between last and new fits\n",
    "        self.diffs = np.array([0,0,0], dtype='float') \n",
    "        #x values for detected line pixels\n",
    "        self.allx = None  \n",
    "        #y values for detected line pixels\n",
    "        self.ally = None\n",
    "        \n",
    "        #distance in meters of vehicle center from the line\n",
    "        self.line_base_pos = None \n",
    "        #radius of curvature of the line in some units\n",
    "        self.radius_of_curvature = None \n",
    "        # previous 10 left lanes\n",
    "        self.left_fit = None\n",
    "        # previous 10 right lanes\n",
    "        self.right_fit = None\n",
    "        \n",
    "        # previous centroid centers\n",
    "        self.r_center = None\n",
    "        self.l_center = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Import everything needed to edit/save/watch video clips\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.misc import imsave\n",
    "\n",
    "def process_image(image):\n",
    "    # NOTE: The output you return should be a color image (3 channel) for processing video below\n",
    "    # TODO: put your pipeline here,\n",
    "    # you should return the final output (image where lines are drawn on lanes)\n",
    "    result = overlay_pipeline(image)\n",
    "    return result\n",
    "\n",
    "def overlay_pipeline(img):\n",
    "    global broken_images\n",
    "    global line\n",
    "    undist_img = undistort_img(img)\n",
    "    # Threshold\n",
    "    combined_binary = threshold_image(undist_img)\n",
    "\n",
    "    # Perspective warp (threshold)\n",
    "    warped_img, M, Minv = warp_image(combined_binary)\n",
    "    \n",
    "    # Sliding window search with centroids\n",
    "    window_centroids = find_window_centroids(warped_img, window_width, window_height, margin, line.l_center, line.r_center)\n",
    "#     window_centroids = find_window_centroids(warped_img, window_width, window_height, margin)\n",
    "    line.l_center, line.r_center = window_centroids[0]\n",
    "    centroid_img = draw_centroids(warped_img, window_centroids)\n",
    "\n",
    "    # Fit polynomial to find lane lines\n",
    "    centy, leftx, rightx = get_centroid_coordinates(window_centroids)\n",
    "    left_fit, right_fit = fit_centroids(centy, leftx, rightx)\n",
    "    \n",
    "    if line.left_fit is not None:\n",
    "        left_fit = np.mean(np.array([left_fit, line.left_fit]), axis=0)\n",
    "    if line.right_fit is not None:\n",
    "        right_fit = np.mean(np.array([right_fit, line.right_fit]), axis=0)\n",
    "    line.left_fit = left_fit\n",
    "    line.right_fit = right_fit\n",
    "    \n",
    "    ploty, left_fitx, right_fitx = fit_coordinates(centroid_img, left_fit, right_fit)\n",
    "    \n",
    "    \n",
    "    # Save any images we do poorly on\n",
    "    lfp = np.poly1d(left_fit)(720)\n",
    "    rfp = np.poly1d(right_fit)(720)\n",
    "    delta = rfp - lfp\n",
    "    if delta > 930 or delta < 850:\n",
    "        broken_images.append((delta, undist_img))\n",
    "#     if lfp < 150 or lfp > 320:\n",
    "#         broken_images.append(undist_img)\n",
    "#     if rfp < 1000 or rfp > 1250:\n",
    "#         broken_images.append(undist_img)\n",
    "    \n",
    "    left_curverad, right_curverad = curvature_meters(centroid_img, centy, leftx, rightx)\n",
    "    avg_curverad = (left_curverad + right_curverad) / 2\n",
    "    prev_radius = line.radius_of_curvature\n",
    "    if prev_radius is not None:\n",
    "        avg_curverad = (prev_radius + avg_curverad) / 2\n",
    "    line.radius_of_curvature = avg_curverad\n",
    "    curve_str = 'Curve Radius: {}m'.format( np.round(avg_curverad, 2))\n",
    "    \n",
    "    meters_off_center = meters_off(centroid_img, left_fit, right_fit)\n",
    "    prev_off_center = line.line_base_pos\n",
    "    if prev_off_center is not None:\n",
    "        meters_off_center = (meters_off_center + prev_off_center) / 2\n",
    "    center_str = 'Meters off center: {}m'.format(np.round(meters_off_center, 2))\n",
    "    line.line_base_pos = meters_off_center\n",
    "    \n",
    "    overlay = overlay_img(warped_img, ploty, left_fitx, right_fitx, Minv, undist_img)\n",
    "    \n",
    "    cv2.putText(overlay,curve_str, (50,50), cv2.FONT_HERSHEY_SIMPLEX, fontScale=1, color=[255, 255, 255], thickness=4)\n",
    "    cv2.putText(overlay,center_str, (50,100), cv2.FONT_HERSHEY_SIMPLEX, fontScale=1, color=[255, 255, 255], thickness=4)\n",
    "    return overlay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = overlay_pipeline(undistorted_imgs[0])\n",
    "plt.imshow(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broken_images = []\n",
    "line = Line()\n",
    "\n",
    "output = 'output_video/project_video_ch_src.mp4'\n",
    "## To speed up the testing process you may want to try your pipeline on a shorter subclip of the video\n",
    "## To do so add .subclip(start_second,end_second) to the end of the line below\n",
    "## Where start_second and end_second are integer values representing the start and end of the subclip\n",
    "## You may also uncomment the following line for a subclip of the first 5 seconds\n",
    "# clip1 = VideoFileClip(\"project_video.mp4\").subclip(5,10)\n",
    "clip1 = VideoFileClip(\"project_video.mp4\")\n",
    "white_clip = clip1.fl_image(process_image) #NOTE: this function expects color images!!\n",
    "%time white_clip.write_videofile(output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('# of images lane detection failed:', len(broken_images))\n",
    "np.save('broken_images/broken_images_delta.npy', broken_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_broken = sorted(broken_images, key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_broken[-9][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(broken_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
